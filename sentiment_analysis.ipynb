{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "_Artificial Intelligence Nanodegree Program | Natural Language Processing_\n",
    "\n",
    "---\n",
    "\n",
    "With the rise of online social media platforms like Twitter, Facebook and Reddit, and the proliferation of customer reviews on sites like Amazon and Yelp, we now have access, more than ever before, to massive text-based data sets! They can be analyzed in order to determine how large portions of the population feel about certain products, events, etc. This sort of analysis is called _sentiment analysis_. In this notebook you will build an end-to-end sentiment classification system from scratch.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this notebook. You will not need to modify the included code beyond what is requested. Sections that begin with '**TODO**' in the header indicate that you need to complete or implement some portion within them. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `# TODO: ...` comment. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions for you to answer which relate to the task and your implementation. Each section where you will answer a question is preceded by a '**Question:**' header. Carefully read each question and provide your answer below the '**Answer:**' header by editing the Markdown cell.\n",
    "\n",
    "> **Note**: Code and Markdown cells can be executed using the **Shift+Enter** keyboard shortcut. In addition, a cell can be edited by typically clicking it (double-click for Markdown cells) or by pressing **Enter** while it is highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Exploring the data!\n",
    "\n",
    "The dataset we are going to use is very popular among researchers in Natural Language Processing, usually referred to as the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/). It consists of movie reviews from the website [imdb.com](http://www.imdb.com/), each labeled as either '**pos**itive', if the reviewer enjoyed the film, or '**neg**ative' otherwise.\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011.\n",
    "\n",
    "We have provided the dataset for you. You can load it in by executing the Python cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='data/imdb-reviews'):\n",
    "    \"\"\"Read IMDb movie reviews from given directory.\n",
    "    \n",
    "    Directory structure expected:\n",
    "    - data/\n",
    "        - train/\n",
    "            - pos/\n",
    "            - neg/\n",
    "        - test/\n",
    "            - pos/\n",
    "            - neg/\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Data, labels to be returned in nested dicts matching the dir. structure\n",
    "    data = {}\n",
    "    labels = {}\n",
    "\n",
    "    # Assume 2 sub-directories: train, test\n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "\n",
    "        # Assume 2 sub-directories for sentiment (label): pos, neg\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            # Fetch list of files for this sentiment\n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            # Read reviews data and assign labels\n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    labels[data_type][sentiment].append(sentiment)\n",
    "            \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "    \n",
    "    # Return data, labels as nested dicts\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "data, labels = read_imdb_data()\n",
    "print(\"IMDb reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "        len(data['train']['pos']), len(data['train']['neg']),\n",
    "        len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded in, let's take a quick look at one of the positive reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i would never have thought that it would be possible to make such an impressive movie without any music. but it is. just the pictures. watch out for that picture: anne talking with that little boy benny 'bout the soul. really strong. might make you feel different.\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['pos'][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one with a negative sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you like to comment on films where the script arrive halfway the movie then this is the one. A setting and acting as in a Porn movie but nothing is happening only some groping and touching of the third kind. Which actually becomes very boring after 45 minutes of touchy feely but no action. A few of the actors I've seen in real x rated movies and there their acting then was a lot better. All the special effects are done by the great \"Rondo\" Whom performs all the magic whit his mind. A cult movie is written on the box. Does that mean that this film is not to be watched at all???<br /><br />Get drunk with some friends and watch this movie on new years eve ore thanks giving.\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['neg'][92])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a wordcloud visualization of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /home/workspace (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/workspace (from wordcloud) (1.14.3)\n",
      "Requirement already satisfied: matplotlib in /home/workspace (from wordcloud) (2.2.2)\n",
      "Requirement already satisfied: pillow in /home/workspace (from wordcloud) (5.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/workspace (from matplotlib->wordcloud) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/workspace (from matplotlib->wordcloud) (1.0.1)\n",
      "Requirement already satisfied: pytz in /home/workspace (from matplotlib->wordcloud) (2018.4)\n",
      "Requirement already satisfied: six>=1.10 in /home/workspace (from matplotlib->wordcloud) (1.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/workspace (from matplotlib->wordcloud) (2.7.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/workspace (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: setuptools in /home/workspace (from kiwisolver>=1.0.1->matplotlib->wordcloud) (39.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing wordcloud\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "sentiment = 'pos'\n",
    "\n",
    "# Combine all reviews for the desired sentiment\n",
    "combined_text = \" \".join([review for review in data['train'][sentiment]])\n",
    "\n",
    "# Initialize wordcloud object\n",
    "wc = WordCloud(background_color='white', max_words=50,\n",
    "        # update stopwords to include common words like film and movie\n",
    "        stopwords = STOPWORDS.update(['br','film','movie']))\n",
    "\n",
    "# Generate and plot wordcloud\n",
    "plt.imshow(wc.generate(combined_text))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the sentiment to `'neg'` and see if you can spot any obvious differences between the wordclouds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Form training and test sets\n",
    "\n",
    "Now that you've seen what the raw data looks like, combine the positive and negative documents to get one unified training set and one unified test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos', 'neg'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While the original 1932 version, with Preston Foster, was good, there\\'s no remake more worthy than this 1959 one, or more impossible to find anywhere, just as I strongly suspect Mickey Rooney to have had something to do with that. Never could a mere performance have ever been so masterfully brilliant, or a script more thought-provoking, as well as an improvement upon the original. Many years after the last of my several viewings of this film, in 1970, I read an article in which Mickey Rooney was recounting a visit he\\'d made to death row, and which had apparently very drastically eliminated whatever sense of personal identification he\\'d felt with people in similar circumstances. The article was about as short as the main character here, and didn\\'t cover much, other than the extent to which his extreme disillusionment with the quality of the inmates themselves had been emphasized, even in language I would not care to explicitly quote here. . . . . One of my main problems with capital punishment is that, of course, it is not evenly, impartially applied, just as many innocent people are far-too-carelessly, thus unnecessarily sent to meet this particular fate. Another problem I have with it is that it is not applied swiftly enough, or, for that matter, even publicly enough! The bible makes a special point, in such cases, about one of the more important purposes of such, as a deterrent, being ineffectually obscured, minus, not only a public viewing, but also the direct participation of all! As for those who claim to prove, statistically, that such is not an effective deterrent? In addition to having a problem about the reliability of their data, I have little if any objectively disprovable doubt many are behind bars now due to the extent that such a deterrent is lacking. However, I do have a problem about the fact that Robert Duvall, in The Apostle, had been punished at all, for his particular \"crime,\" or that the only hope of leniency for one such as he would have to be based on a \"temporary insanity\" defense, as though that would serve as the only acceptable excuse in his kind of case. . . . In addition to various other questions concerning the motives of Mickey Rooney for that particular visit he\\'d recounted, and about the answers to which I can only try to speculate, I suspect the main one had been of a decidedly religious nature. I don\\'t know exactly when he\\'d become the professing Christian he now makes it a special point, whenever possible, to emphasize that he is; but, as anybody should be well-aware, this particular category of people tends to be the most vehemently out for blood, when it comes to extracting an eye for an eye. However, I have no particular bone of contention concerning that, per se, just as there\\'s no doubt, scripturally speaking, that not all, and perhaps not even most, shall be spared the same ultimate fate, at the hands of the Lord Himself, as a result of His sacrifice on the cross. However, there is a problem, for me, about the spirit or attitude with which most professing Christians emphasize their enthusiasm for capital punishment; for, contrary to the Lord Himself, who would love to see everybody saved (Ezekiel 18:32) (II Peter 3:9), they seem to go vindictively out of their way to find reasons to condemn! . . . What most people, on either side of this superlatively ever-burning issue, cannot appear to sufficiently appreciate, is that the Lord is as dynamically and elusively soft in nature as He is hard. The two sides of His nature appear to be so inherently incompatible as to render Him mentally deranged, at least by any strictly human reckoning. Yet, regardless of how harrowingly ungraspable this miraculously dynamic blending of the water and oil in His nature surely is, there can be no doubt that anything short of it, or anything fanatically and characteristically on either one side or the other of this equation, falls inadequately and unacceptably short of the entire judicial truth. Indeed, I\\'ve seen the most blood-curdling thirst for the same come out, self-contradictorily enough, on far-too-many occasions, whenever the categorically anti-death penalty advocates are confronted, even in the most rationally well-balanced ways, with the fact that, although the Lord died for everybody, not all are thereby going to be saved. After-all, in order to receive absolution, one must, to repeat the same term, reach out and receive it, that is, repent (Luke 13:3-5). Could anything make more sense? . . . But, then, what about the Lord\\'s command to forgive, even in the case of one\\'s enemies, of those who despise and persecute you without a just cause or provocation? One of the far-too-prevailing difficulties with this kind of sentimentality, as popularly misinterpreted, is the way it obscuringly over-simplifies the real meaning of forgiveness. The act of forgiveness does not, in itself, mean the same thing as unconditionally excusing the one being forgiven. When one takes a clearly sober, rationally well-balanced view here, from the perspective of God\\'s own attitude, all it actually amounts to is a fervent wish that the one forgiven will ultimately succeed at finding his way, seeing the light, and being granted mercy. This attitude is, of course, the very opposite of, say, that of Jonah, who actually resented it when God told him that his preaching to the people of Nineveh would result in their repentance. Jonah didn\\'t want them to repent, but vindictively desired that they be destroyed. How self-righteously, cold-bloodedly like unto most professing Christians he was, save that even his reasons were undoubtedly better than most! I envy Jonah almost as much as he would me! However, minus the repentance of the one being forgiven, any forgiveness he may receive from a genuine Christian is not going to do him any good. In such a case, the only one to benefit is the real Christian himself!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][\"pos\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    # TODO: Combine positive and negative reviews and labels\n",
    "    data_train = data['train'][\"pos\"] + data[\"train\"][\"neg\"]\n",
    "    data_test = data['test'][\"pos\"] + data[\"test\"][\"neg\"]\n",
    "    labels_train = labels['train'][\"pos\"] + labels[\"train\"][\"neg\"]\n",
    "    labels_test = labels['test'][\"pos\"] + labels[\"test\"][\"neg\"]\n",
    "    \n",
    "    # TODO: Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train , labels_train = shuffle(data_train,labels_train)\n",
    "    data_test , labels_test = shuffle(data_test,labels_test)\n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test\n",
    "\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = prepare_imdb_data(data)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(data_train), len(data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Preprocessing\n",
    "\n",
    "As you might have noticed in the sample reviews, our raw data includes HTML. Therefore there are HTML tags that need to be removed. We also need to remove non-letter characters, normalize uppercase letters by converting them to lowercase, tokenize, remove stop words, and stem the remaining words in each document.\n",
    "\n",
    "### TODO: Convert each review to words\n",
    "\n",
    "As your next task, you should complete the function `review_to_words()` that performs all these steps. For your convenience, in the Python cell below we provide you with all the libraries that you may need in order to accomplish these preprocessing steps. Make sure you can import all of them! (If not, pip install from a terminal and run/import again.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# BeautifulSoup to easily remove HTML tags\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "# RegEx for removing non-letter characters\n",
    "import re\n",
    "\n",
    "# NLTK library for the remaining steps\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")   # download list of stopwords (only once; need not run it again)\n",
    "from nltk.corpus import stopwords # import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'em test em br br test would make b great b movi review'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def review_to_words(review):\n",
    "    \"\"\"Convert a raw review string into a sequence of words.\"\"\"\n",
    "    \n",
    "    # TODO: Remove HTML tags and non-letters,\n",
    "    #       convert to lowercase, tokenize,\n",
    "    #       remove stopwords and stem\n",
    "\n",
    "    # Return final list of words\n",
    "    for i in range(0,len(review)):\n",
    "        words = re.sub(r\"[^A-Za-z0-9]\",' ',review)\n",
    "        words = words.lower()\n",
    "        words = words.split()\n",
    "        words = [stemmer.stem(word) for word in words if word not in stopwords.words(\"english\")]\n",
    "        words = \" \".join(words)\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "review_to_words(\"\"\"This is just a <em>test</em>.<br/><br />\n",
    "But if it wasn't a test, it would make for a <b>Great</b> movie review!\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function `review_to_words()` fully implemeneted, we can apply it to all reviews in both training and test datasets. This may take a while, so let's build in a mechanism to write to a cache file and retrieve from it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n",
      "\n",
      "--- Raw review ---\n",
      "Forest Whitaker's performance is all the more impressive for making it almost worthwhile to sit through this dreck. \"Historical fiction\" does not justify changing history. The absurdity begins from the ground up with the imaginary lead character played by James McAvoy. To create a fictional observer for the purpose of giving the reader a point of connection in the book is regrettable, regrettable that white people can't just read a book without a white protagonist to connect to, but at least he was placed in a somewhat passive role. Making up a fake historical actor and crediting this fiction with exposing Amin to the world is irresponsible, lazy and stupid. Not making the actions of this creation believable or even sane is just criminal, and has opened the door for movies like the one they're planning to make with Leonardo DiCaprio as a heroic Enron whistleblower who NEVER EXISTED. The logic of the world does not apply in this film where some Scottish kid thinks its okay to sleep with the wife of a murderous dictator. It doesn't apply where the wife of the dictator desires to sleep with some stupid scrawny irresponsible white boy. For that matter EVERYBODY is lined up to sleep with this scrawny, irresponsible, arrogant white boy, he even has Gillian Anderson licking her comely chops.<br /><br />Let me declare, I do not like James McAvoy. I'm not sure what it is about him, but I thought his Mr. Tumnus in Narnia was creepy and pervy. I think that Kerry Washington would never look twice at him so I can't believe that the wife of a powerful dangerous man like Amin would risk and lose her life for him. I don't believe him as a Doctor, and I just don't see the appeal. His character seems to have far more arrogance than would make sense, and trying to make him look like a badass in shooting the cow was just...there's that word again...absurd. Think about it, you are watching all of these characters bend themselves into knots in order to accommodate this unbelievable main character and there never was such a guy.<br /><br />Gillian Anderson looks incredible and sounds more British than most Brits. Whitaker gives a great expansive magnetic performance, and highlighted, with his incomprehensible pre-Oscar speeches, just how much he was acting. It's a shame the film around him had no reason to exist.\n",
      "\n",
      "--- Preprocessed words ---\n",
      "['blond', 'blonder', 'pamela', 'anderson', 'denis', 'richard', 'almost', 'everi', 'scene', 'want', 'movi', 'utterli', 'unreason', 'feel', 'like', 'late', 'era', 'carri', 'seri', 'longer', 'blaze', 'trail', 'still', 'funni', 'think', 'behind', 'england', 'far', 'mark', 'pamela', 'denis', 'bubbl', 'charm', 'clearli', 'awar', 'masterpiec', 'make', 'although', 'give', 'lot', 'thing', 'told', 'like', 'support', 'cast', 'energet', 'even', 'particularli', 'good', 'see', 'coupl', 'duff', 'turn', 'movi', 'alreadi', 'practic', 'forgotten', 'make', 'much', 'differ', 'anyth', 'smile', 'realli', 'think', 'blond', 'blonder', 'ace', 'hope', 'hate']\n",
      "\n",
      "--- Label ---\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_train = list(map(review_to_words, data_train))\n",
    "        words_test = list(map(review_to_words, data_test))\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "words_train, words_test, labels_train, labels_test = preprocess_data(\n",
    "        data_train, data_test, labels_train, labels_test)\n",
    "\n",
    "# Take a look at a sample\n",
    "print(\"\\n--- Raw review ---\")\n",
    "print(data_train[1])\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(words_train[1])\n",
    "print(\"\\n--- Label ---\")\n",
    "print(labels_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extracting Bag-of-Words features\n",
    "\n",
    "Now that each document has been preprocessed, we can transform each into a Bag-of-Words feature representation. Note that we need to create this transformation based on the training data alone, as we are not allowed to peek at the testing data at all!\n",
    "\n",
    "The dictionary or _vocabulary_ $V$ (set of words shared by documents in the training set) used here will be the one on which we train our supervised learning algorithm. Any future test data must be transformed in the same way for us to be able to apply the learned model for prediction. Hence, it is important to store the transformation / vocabulary as well.\n",
    "\n",
    "> **Note**: The set of words in the training set may not be exactly the same as the test set. What do you do if you encounter a word during testing that you haven't seen before? Unfortunately, we'll have to ignore it, or replace it with a special `<UNK>` token.\n",
    "\n",
    "### TODO: Compute Bag-of-Words features\n",
    "\n",
    "Implement the `extract_BoW_features()` function, apply it to both training and test datasets, and store the results in `features_train` and `features_test` NumPy arrays, respectively. Choose a reasonable vocabulary size, say $|V| = 5000$, and keep only the top $|V|$ occuring words and discard the rest. This number will also serve as the number of columns in the BoW matrices.\n",
    "\n",
    "> **Hint**: You may find it useful to take advantage of `CountVectorizer` from scikit-learn. Also make sure to pickle your Bag-of-Words transformation so that you can use it in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features to cache file: bow_features.pkl\n",
      "Vocabulary: 5000 words\n",
      "Sample words: ['fabul', 'mutil', 'sherlock', 'parallel', 'movement', 'be', 'fox', 'cheek']\n",
      "\n",
      "--- Preprocessed words ---\n",
      "['kid', '50', '60', 'anyth', 'connect', 'disney', 'definit', 'great', 'happen', 'abl', 'get', 'actor', 'actress', 'want', 'best', 'time', 'somehow', 'disney', 'manag', 'screw', 'thing', 'spite', 'abund', 'resourc', 'disney', 'afford', 'best', 'writer', 'best', 'produc', 'director', 'still', 'screw', 'thing', 'movi', 'crap', 'sad', 'thing', 'suspect', 'disney', 'arrog', 'even', 'know', 'movi', 'good', 'bad', 'due', 'talent', 'actor', 'even', 'give', '3', '10']\n",
      "\n",
      "--- Bag-of-Words features ---\n",
      "[0 0 0 ... 0 0 0]\n",
      "\n",
      "--- Label ---\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # TODO: Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features= vocabulary_size,preprocessor= lambda x : x , tokenizer= lambda x: x)\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # TODO: Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.fit_transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary\n",
    "\n",
    "\n",
    "# Extract Bag of Words features for both training and test datasets\n",
    "features_train, features_test, vocabulary = extract_BoW_features(words_train, words_test)\n",
    "\n",
    "# Inspect the vocabulary that was computed\n",
    "print(\"Vocabulary: {} words\".format(len(vocabulary)))\n",
    "\n",
    "import random\n",
    "print(\"Sample words: {}\".format(random.sample(list(vocabulary.keys()), 8)))\n",
    "\n",
    "# Sample\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(words_train[5])\n",
    "print(\"\\n--- Bag-of-Words features ---\")\n",
    "print(features_train[5])\n",
    "print(\"\\n--- Label ---\")\n",
    "print(labels_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to visualize the Bag-of-Words feature vector for one of our training documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the BoW feature vector for a training document\n",
    "plt.plot(features_train[5,:])\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Reflecting on Bag-of-Words feature representation\n",
    "\n",
    "What is the average sparsity level of BoW vectors in our training set? In other words, on average what percentage of entries in a BoW feature vector are zero?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "...\n",
    "\n",
    "### Zipf's law\n",
    "\n",
    "[Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law), named after the famous American linguist George Zipf, is an empirical law stating that given a large collection of documents, the frequency of any word is inversely proportional to its rank in the frequency table. So the most frequent word will occur about twice as often as the second most frequent word, three times as often as the third most frequent word, and so on. In the figure below we plot number of appearances of each word in our training set against its rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find number of occurrences for each word in the training set\n",
    "word_freq = features_train.sum(axis=0)\n",
    "\n",
    "# Sort it in descending order\n",
    "sorted_word_freq = np.sort(word_freq)[::-1]\n",
    "\n",
    "# Plot \n",
    "plt.plot(sorted_word_freq)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Number of occurrences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([51695, 48190, 27741, ...,    62,    62,    62], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Zipf's law\n",
    "\n",
    "What is the total number of occurrences of the most frequent word? What is the the total number of occurrences of the second most frequent word? Do your numbers follow  Zipf's law? If not, why?\n",
    "\n",
    "#### Answer:\n",
    "1st most frequent = 51695\n",
    "2nd most frequent = 48190\n",
    "NO\n",
    "...\n",
    "\n",
    "### TODO: Normalize feature vectors\n",
    "\n",
    "Bag-of-Words features are intuitive to understand as they are simply word counts. But counts can vary a lot, and potentially throw off learning algorithms later in the pipeline. So, before we proceed further, let's normalize the BoW feature vectors to have unit length.\n",
    "\n",
    "This makes sure that each document's representation retains the unique mixture of feature components, but prevents documents with large word counts from dominating those with fewer words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pr\n",
    "\n",
    "# TODO: Normalize BoW features in training and test set\n",
    "\n",
    "normalize_train = pr.normalize(features_train,axis=1)\n",
    "normalize_test = pr.normalize(features_test,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Classification using BoW features\n",
    "\n",
    "Now that the data has all been properly transformed, we can feed it into a classifier. To get a baseline model, we train a Naive Bayes classifier from scikit-learn (specifically, [`GaussianNB`](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)), and evaluate its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GaussianNB] Accuracy: train = 0.8198, test = 0.49596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# TODO: Train a Guassian Naive Bayes classifier\n",
    "clf1 = GaussianNB()\n",
    "clf1.fit(normalize_train,labels_train)\n",
    "\n",
    "# Calculate the mean accuracy score on training and test sets\n",
    "print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        clf1.__class__.__name__,\n",
    "        clf1.score(normalize_train,labels_train),\n",
    "        clf1.score(normalize_test,labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree-based algorithms often work quite well on Bag-of-Words as their highly discontinuous and sparse nature is nicely matched by the structure of trees. As your next task, you will try to improve on the Naive Bayes classifier's performance by using scikit-learn's Gradient-Boosted Decision Tree classifer.\n",
    "\n",
    "### TODO: Gradient-Boosted Decision Tree classifier\n",
    "\n",
    "Use [`GradientBoostingClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) from scikit-learn to classify the BoW data. This model has a number of parameters. We use default parameters for some of them and pre-set the rest for you, except one: `n_estimators`. Find a proper value for this hyperparameter, use it to classify the data, and report how much improvement you get over Naive Bayes in terms of accuracy.\n",
    "\n",
    "> **Tip**: Use a model selection technique such as cross-validation, grid-search, or an information criterion method, to find an optimal value for the hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GradientBoostingClassifier] Accuracy: train = 0.71848, test = 0.4998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "n_estimators = 8\n",
    "\n",
    "def classify_gboost(X_train, X_test, y_train, y_test):        \n",
    "    # Initialize classifier\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "\n",
    "    # TODO: Classify the data using GradientBoostingClassifier\n",
    "    clf.fit(X_train,y_train)\n",
    "    # TODO(optional): Perform hyperparameter tuning / model selection\n",
    "    \n",
    "    # TODO: Print final training & test accuracy\n",
    "    print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        clf.__class__.__name__,\n",
    "        clf.score(X_train,y_train),\n",
    "        clf.score(X_test,y_test)))\n",
    "    # Return best classifier model\n",
    "    return clf\n",
    "\n",
    "\n",
    "clf2 = classify_gboost(normalize_train, normalize_test, labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Adverserial testing\n",
    "\n",
    "Write a short movie review to trick your machine learning model! That is, a movie review with a clear positive or negative sentiment that your model will classify incorrectly.\n",
    "\n",
    "> **Hint**: You might want to take advantage of the biggest weakness of the Bag-of-Words scheme!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- My review ---\n",
      "The Movie Was Awesome !!!! - every thing was GREAT\n",
      "\n",
      "--- Preprocessed words ---\n",
      "movi awesom everi thing great\n",
      "\n",
      "True sentiment: pos, predicted sentiment: pos\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a sample review and set its true sentiment\n",
    "my_review = \"The Movie Was Awesome !!!! - every thing was GREAT\"\n",
    "true_sentiment = 'pos'  # sentiment must be 'pos' or 'neg'\n",
    "\n",
    "# TODO: Apply the same preprocessing and vectorizing steps as you did for your training data\n",
    "my_words = review_to_words(my_review)\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary,\n",
    "                             preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
    "my_bow_features = vectorizer.transform([my_words]).toarray()  # pass an array of 1 element\n",
    "\n",
    "# TODO: Then call your classifier to label it\n",
    "predicted_sentiment = clf2.predict(my_bow_features)[0]\n",
    "print(\"--- My review ---\")\n",
    "print(my_review)\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(my_words)\n",
    "print(\"\\nTrue sentiment: {}, predicted sentiment: {}\".format(true_sentiment, predicted_sentiment))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "There are several ways in which you can build upon this notebook. Each comes with its set of challenges, but can be a rewarding experience.\n",
    "\n",
    "- The first thing is to try and improve the accuracy of your model by experimenting with different architectures, layers and parameters. How good can you get without taking prohibitively long to train? How do you prevent overfitting?\n",
    "\n",
    "- Then, you may want to deploy your model as a mobile app or web service. What do you need to do in order to package your model for such deployment? How would you accept a new review, convert it into a form suitable for your model, and perform the actual prediction? (Note that the same environment you used during training may not be available.)\n",
    "\n",
    "- One simplification we made in this notebook is to limit the task to binary classification. The dataset actually includes a more fine-grained review rating that is indicated in each review's filename (which is of the form `<[id]_[rating].txt>` where `[id]` is a unique identifier and `[rating]` is on a scale of 1-10; note that neutral reviews > 4 or < 7 have been excluded). How would you modify the notebook to perform regression on the review ratings? In what situations is regression more useful than classification, and vice-versa?\n",
    "\n",
    "Whatever direction you take, make sure to share your results and learnings with your peers, through blogs, discussions and participating in online competitions. This is also a great way to become more visible to potential employers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
